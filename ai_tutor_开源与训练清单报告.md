# AI Tutor 开源库可直接使用与需训练模型清单报告

本报告基于现有实施方案与调研结论，给出两类清单：
1) 可直接集成/使用的开源库与组件  
2) 需要基于自有数据进行训练或微调的模型模块

---

## 一、可直接使用的开源库清单（不需要自有数据训练）

### 1) 数据处理与ETL
- `ffmpeg`：音视频抽取、转码、切分
- `pandas`：日志清洗与结构化
- `pyarrow`：列式存储（Parquet/Arrow）

### 2) 时间对齐与图像处理
- `opencv-python`：关键帧差分、OCR前处理
- `rapidfuzz`：文本相似度与模糊匹配
- `jiwer`：ASR WER 评估

### 3) ASR 与流式处理框架
- `faster-whisper`：离线ASR推理
- `whisper.cpp`：流式/边缘ASR推理
- `webrtcvad` 或 `silero-vad`：语音活动检测（VAD）

### 4) 说话人分离与角色识别
- `pyannote.audio`：说话人分离推理

### 5) 音频与视觉特征抽取
- `openSMILE`：音频情感/韵律特征
- `librosa`：音频基础特征
- `mediapipe`：人脸/姿态特征提取
- `openface`：表情/情绪相关特征提取

### 6) 报告与可视化
- `jinja2`：报告模板生成
- `plotly` 或 `matplotlib`：图表输出

### 7) 后端与实时服务
- `FastAPI`：API与WebSocket
- `SQLAlchemy`：数据库ORM
- `redis` / `redis-streams`：实时缓存/流式缓冲

### 8) 前端与UI
- `Next.js` + `React`：Web端
- `Tailwind`：样式
- `Plotly` 或 `ECharts`：可视化组件

---

## 二、需要使用自有数据训练或微调的模型清单

### 1) 知识追踪（KT）
- **模型**：BKT / DKT / SAKT / AKT / IKT
- **原因**：需要与本项目的知识点体系、题目标签、学生行为序列匹配
- **训练数据**：题目-知识点映射、答题正确性、时间间隔、历史序列

### 2) 参与度 / 情绪识别
- **模型**：音频情感 + 视觉表情/姿态融合分类器
- **原因**：课堂场景与学生年龄段差异大，需要本地标注或弱监督
- **训练数据**：课堂音视频片段 + 参与度/情绪标签

### 3) 课堂互动/策略推荐
- **模型**：规则策略（初期可直接用），后续可训练 Contextual Bandit / RL
- **原因**：策略效果依赖具体课堂场景与老师反馈
- **训练数据**：干预触发记录 + 反馈效果（课堂/教师评价）

### 4) 发音评估（若上线）
- **模型**：音素级别打分或ASR对齐模型
- **原因**：需要匹配具体教学内容与发音标准
- **训练数据**：学生音频 + 标准文本/音素标注

### 5) 说话人角色映射（教师/学生识别）
- **模型**：基于说话人向量/聚类的角色分类
- **原因**：课堂录音环境与设备差异，需要本地校准
- **训练数据**：带角色标签的语音片段

---

## 三、可直接使用但建议轻量校准/微调的组件

- **ASR 模型**：可直接使用 Whisper 系列，但在儿童语音/噪声课堂场景建议少量本地微调或词表适配。
- **说话人分离**：可直接使用 `pyannote.audio` 推理，但建议做本地阈值校准以提升分离稳定性。
- **情绪/参与度特征**：特征抽取无需训练，但分类器需要本地训练。

---

## 四、结论

- **可直接使用**：数据处理、对齐、ASR推理、说话人分离推理、特征抽取、后端与前端框架等基础工具链。
- **需要自有数据训练**：知识追踪、参与度/情绪识别、课堂策略推荐、发音评估与角色映射模型。
- **建议策略**：先用成熟开源库打通管道与报告，再逐步引入本地训练模型以提升准确性。

